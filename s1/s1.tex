\documentclass[a4paper, 12pt, twoside]{article}
    % General document formatting
    \usepackage[a4paper,
            left=20mm, right=20mm,
			top=20mm, bottom=20mm]{geometry}
    \usepackage[parfill]{parskip}
    \usepackage[utf8]{inputenc}

    % Related to math
    \usepackage{amsmath,amssymb,amsfonts,amsthm}
    \usepackage{mathtools}
    \newcounter{tagno}
    \setcounter{tagno}{0}
    \newcommand{\mytag}[1]{\tag{\thetagno} \label{#1} \stepcounter{tagno}}

    \usepackage{authblk}
    \title{Supplementary 1: Simulation for validating $R_t$ estimates, additional results and sensitivity analyses}
    \author[1,2]{Robert Challen}
    \author[1,2]{Leon Danon}
    \affil[1]{AI4CI, University of Bristol, Bristol, UK.}
    \affil[2]{Department of Engineering Mathematics, University of Bristol, Bristol, UK.}
    \date{}                     %% if you don't need date to appear
    \setcounter{Maxaffil}{0}
    \renewcommand\Affilfont{\itshape\small}

    % keep figures in same section
    \usepackage{placeins}
    \let\Oldsection\section
    \renewcommand{\section}{\FloatBarrier\Oldsection}
    \let\Oldsubsection\subsection
    \renewcommand{\subsection}{\FloatBarrier\Oldsubsection}
    \let\Oldsubsubsection\subsubsection
    \renewcommand{\subsubsection}{\FloatBarrier\Oldsubsubsection}
    \DeclareMathOperator{\sgn}{sgn}

    % for \ie \eg
    \usepackage{xspace}
    \newcommand*{\eg}{e.g.\@\xspace}
    \newcommand*{\ie}{i.e.\@\xspace}
    \newcommand*{\nb}{N.b.\@\xspace}

    \usepackage{booktabs}
    \usepackage{multirow}
    \usepackage[table,xcdraw]{xcolor}

    % cite package, to clean up citations in the main text. Do not remove.
%     \usepackage{cite}
%     \bibliographystyle{plain}
    \usepackage[hidelinks]{hyperref}
    \usepackage[numbers]{natbib}
    \bibliographystyle{unsrturl}

\begin{document}

\maketitle

\section{Introduction}

In the main paper we develop an algorithm for the estimation of the reproduction number $R_t$, and show that it produces sensible estimates for $R_t$ by using an infectious disease outbreak simulation parametrised with $R_t$. This supplementary provides detail the on the approach to simulation and validation of the $R_t$ method described in the main paper, and provides additional figures and the results of sensitivity analyses. Further detail on the performance metrics reported is available in Supplementary Appendix 2.

\section{Simulation}

The basis for the simulation and subsequent estimation of $R_t$ relies on an infectivity profile, that defines the delay distribution of secondary cases given primary infections \cite{gostic2020,thompson2019}. In a real world situation this delay distribution may be inferred from data and associated with uncertainty. To replicate this we generate a set of infectivity profiles based on a gamma distribution with uncertain parameters as described in figure 1. The average value of this distribution is used as the ``true'' value and used to generate the simulated data, combined with an $R_t$ parameter. The multiple possible infectivity profiles in Fig~\ref{fig:S1} represent the uncertainty we would expect in a real scenario, when infectivity profile is inferred from data. Therefore all 100 infectivity profiles are used in estimating the reproduction number, and contribute to its uncertainty, however the same central infectivity profile is used during all simulation runs.

\begin{figure}[h!]
\centering
  \includegraphics{fig/fig1-simulated-inf-prof}
  \caption{A simulated set of 100 infectivity profiles generated by the R package `EpiEstim`, using mean of 5 ($\pm$ 1) days and SD of 3 ($\pm$ 1)  days, as an offset gamma distribution, discretised on daily intervals, and truncated at 14 days. The average of these infectivity profiles is regarded as the ``true'' value and is highlighted in blue. }
\label{fig:S1}
\end{figure}

The simulation is a stochastic discrete time branching process model where each node in the model is an infected individual. The model is parameterised by a time varying reproduction number ($R_t$), and an infectivity profile ($\omega$). The model is seeded with an initial set of 30 infected individuals. Growth of the model is not constrained to a population size. The probability that any infected individual ($X_{i,t}$) who was infected at time $t$ generates secondary infections  ($X_{j,i,t+\tau}$) on a subsequent day $t+\tau$ is determined by the reproduction number on that day ($R_{t+\tau}$) and the infectivity profile of the day since primary infection ($\omega_{\tau}$).

\begin{equation*}
\begin{aligned}
E(X_{j,i,t}|X_{i,t},\tau) = \omega_{\tau} \times R_{t+\tau} \\
\end{aligned}
\mytag{eq:simulation-setup}
\end{equation*}

From the branching process model the count of simulated infections for each day ($I_{t}$) is the ``true count'' of infections. For the purposes of the simulation this true count is said to be partly observed with a different probability of ascertainment each day ($P_{asc}$). This probability is a random sample from a Beta distributed quantity with mean of $0.7$ and different degrees of dispersion as outlined in the table below (expressed as coefficients of variation), the result of which is an observed count of infections $O_t$:

\begin{equation*}
\begin{aligned}
I_{t} &= |X_{t}| \\
O_{t} &\sim Binom(I_{t}, P_{asc}) \\
P_{asc} &\sim Beta(\alpha, \beta)
\end{aligned}
\mytag{eq:simulation-setup-2}
\end{equation*}

This observed case count is a fraction of the same infection counts for each scenario replication and differs only by the day to day variation in ascertainment.

\begin{center}
\begin{table}
  \caption{Parameters of a Beta distribution ($\alpha$ and $\beta$) which decide the day to day ascertainment rate of the simulations ($P_{asc}$)}
  \centering
  \begin{tabular}{lllll}
  \hline
  dispersion & $\alpha$ & $\beta$ & mean: $\mu$ & coefficient of variation: $\frac{\sigma}{\mu}$\\
  \hline
  low & 302.63 & 129.7 & 0.7 & $3.145 \times 10^{-02}$\\
  medium & 18.25 & 7.83 & 0.7 & $1.258 \times 10^{-01}$\\
  high & 5.49 & 2.35 & 0.7 & $2.201 \times 10^{-01}$\\
  \hline
  \end{tabular}
\end{table}
\end{center}

\clearpage

\section{Validation}

We compare different methods for estimating $R_t$, `EpiEstim' and `$R_t$ from incidence' as described in the main paper. The different $R_t$ estimators are subject to varying delays as seen in Fig 1 in the main paper and Fig~\ref{fig:S2} in this appendix. The lags can be quantified by estimating the reproduction number for a well characterised dataset generated with a periodically varying reproduction number. The minimum root mean squared error between prediction and known $R_t$ is taken as the estimate lag. We see the lag introduced by `EpiEstim' is half of the window size \cite{gostic2020,parag2021} using the standard interpretation of `EpiEstim' as the $R_t$ estimate at being at the end of the smoothing window.

\begin{figure}[h!]
\centering
  \includegraphics{fig/fig2-lag-plot}
  \caption{Lags in $R_t$ estimation methods. `EpiEstim' integrates information over a window, during which $R_t$ is assumed constant. By convention the time point for the estimate is taken to be the end of the window and therefore the estimate is lagged (left panel). The $R_t$ from modelled incidence method described here is only affected by delays in the incidence estimation but the method we chose is not lagged (right panel).}
\label{fig:S2}
\end{figure}

\clearpage

The testing consisted of 50 replicates of 5 randomly generated scenarios, each with 3 levels of ascertainment dispersion. The various scenarios and the highest and lowest $R_t$ estimates for the scenario replications are shown in Fig~\ref{fig:S3}. During the very early phase of an epidemic $R_t$ estimators results are typically unreliable. When comparing estimators we only use the estimates from day 20 to 80, when they have established a pattern. The scenarios (which were randomly generated) are quite similar apart from simulation 5 which has smaller variation.

\begin{figure}[h!]
\centering
  \includegraphics{fig/fig3-scenario-estimates}
  \caption{The five scenarios tested. Each scenario (rows) had 50 replications and 3 levels of ascertainment with different dispersion applied to them (columns). The coloured traces show the replicate resulting in the highest and lowest $R_t$ estimates for each scenario, for each method. During validation the first 20 days worth of estimates were discarded as the lead in to the time series predictions suffers from a specific inaccuracy due to the incomplete nature of the data informing the renewal equation in this early phase.}
\label{fig:S3}
\end{figure}

\clearpage

\section{Additional results}

The selection of metrics for validation are introduced in the main paper, and described in detail in Supplementary appendix 2. In this supplementary we show results faceted by scenario and ascertainment bias. These additional results give us some sense as to whether there are specific features of the testing scenario that favour one estimator over the other.

Fig~\ref{fig:S4} estimator quality stratified by simulation scenario and ascertainment noise shows similar patterns to the main paper. In panel A the mean CRPS shows that in all scenarios the `$R_t$ Locfit' estimator scores lower (better) and in panel B the proportional bias is usually closer to zero (less biased). `EpiEstim' and `$R_t$ GAM' are consistently sharper than `$R_t$ Locfit' meaning that it produces results with narrower confidence intervals. In the presence of bias this is not always desirable. Calibration is assessed in panel D and `$R_t$ from incidence' estimates are closer to the ideal value of 0.5 in most scenarios, except for scenario 5, this suggests that in scenarios 1-4 `EpiEstim' and `$R_t$ GAM' are over confident, whereas in scenario 5 `$R_t$ Locfit' is excessively cautious. Similarly the other measure of calibration, the adjusted PIT-Wasserstein score (details in supplementary 2) is better (lower) in `$R_t$ Locfit' in scenarios 1-4.

Scenario 5 was noted above to have less variation in high and low values. This suggests that `EpiEstim' and `$R_t$ GAM' are more accurate when the time series is more stable. Step changes should be equally challenging for both methods to adapt to (which is why they were chosen), and have relevance to behvioural interventions such as lockdown, but other validation scenarios such as smoothly changing $R_t$ values could be considered.

\begin{figure}[h!]
\centering
  \includegraphics{fig/fig4-metrics-by-scenario}
  \caption{Mean Continuous Ranked Probability Scores (panel A), proportional bias as a percent (panel B), 50\% prediction interval width (panel C), 50\% coverage probability (panel D) and Probability integral transform (PIT) histogram Wasserstein distance from uniform (Panel E), stratified by scenario and ascertainment noise. Lower values are better in all metrics apart from the 50\% coverage probability (panel D) which is ideally 0.5. Lower values of the prediction interval width are only unequivocally better in an unbiased and well calibrated estimator, otherwise a trade off between sharpness, bias and calibration is required.}
\label{fig:S4}
\end{figure}

\clearpage

\section{Sensitivity analyses}

In the first sensitivity analysis we re-ran the 2 estimators with different configuration. In this case we narrowed the data window on which the estimates are based. This made all estimators less certain and more reactive to changes in the data (less smooth). These effects are seen in the prediction time series in Fig~\ref{fig:S5} panel A. In terms of the quality metrics found in in Panels B-G, the the estimators show a similar behaviour to the main analysis in terms of bias and sharpness (Panel C and D), but `EpiEstim' is now slightly better calibrated with the increased unceratinty due to the shorter data window (panel E). The overall performance as measured by CRPS is best for `$R_t$ GAM' measured by the CRPS (panel B). The calibration suggests that `$R_t$ Locfit' is now tending towards being excessively conservative, except when ascertainment noise is particularly high (Panel E), whereas other estimates remain excessively certain. In terms of overall misclassification at the threshold of $R_t=1$ there is still not much to choose between the `EpiEstim' and `$R_t$ Locfit'm, but `$R_t$ GAM' is better than both.

\begin{figure}[h!]
\centering
  \includegraphics{fig/fig5-7-day-scenario}
  \caption{Sensitivity analysis of `EpiEstim', and `$R_t$ from incidence' estimates derived from 7 day data windows, as opposed to 14, as described in the main paper, and are shown in Panel A for each of 5 scenarios (highest and lowest replicates only). The figure compares the continuous ranked probability score (CRPS) - lower is better, panel B; the average proportional bias, in panel C - zero is best; The 50\% prediction interval width which measures estimator sharpness and lower is better, so long as the estimator is unbiased and well calibrated (panel E); the probability of 50\% coverage (ideal value is 0.5, panel E), and adjusted probability integral transform (PIT) Wasserstein metric (lower is better, panel F); the probability of misclassifying at the threshold of $R_t=1$ is shown in panel G (lower is better).}
\label{fig:S5}
\end{figure}

In the second sensitivity analysis (see Fig~\ref{fig:S6}) we repeat the analysis on the main paper but without adjusting for estimator lag before calculating the metrics. This does not change the findings in the main paper, as if anything it penalises `EpiEstim' unfairly due to fact that the `$R_t$ from incidence' does not suffer from lag.

\begin{figure}[h!]
\centering
  \includegraphics{fig/fig6-not-lagged-scenario}
  \caption{Sensitivity analysis of `EpiEstim', and `$R_t$ from incidence' estimates when neither estimate is corrected for lag, are shown in Panel A for each of 5 scenarios (highest and lowest replicates only). The figure compares the continuous ranked probability score (CRPS) - lower is better, panel B; the average proportional bias, in panel C - zero is best; The 50\% prediction interval width which measures estimator sharpness and lower is better, so long as the estimator is unbiased and well calibrated (panel E); the probability of 50\% coverage (ideal value is 0.5, panel E), and adjusted probability integral transform (PIT) Wasserstein metric (lower is better, panel F); the probability of misclassifying at the threshold of $R_t=1$ is shown in panel G (lower is better).}
\label{fig:S6}
\end{figure}

\bibliography{../main/refs}

\end{document}

\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{unsrturl}
\@writefile{toc}{\contentsline {section}{\numberline {1}Background}{1}{section.1}\protected@file@percent }
\citation{anderson1996,bosse2024,bosse2023,brocker2008,gneiting2007}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces An estimator generates probabilistic predictions for any given true values (blue line), based on data (not shown). The estimator has an inherent propensity for error (red curve). For any given true value the central value of a probabilistic prediction (black curve) may not match the true value, and if this is consistently over many predictions, the estimator may be regarded as biased. The estimator also generates a measure of uncertainty of the probabilistic prediction (black bars). Over many predictions the distribution of this uncertainty is a measure of estimator sharpness (yellow curve). The sharpness ideally matches the spread of the estimator's error function. If, as is shown here, the estimators sharpness is narrower than the error function spread then any single prediction is likely to be over-confident. This is the result of a mis-calibrated and over confident estimator. The ratio of the error function standard deviation and the median of the estimator spread distribution (sharpness) is the true value of the calibration.}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:S2.1}{{1}{2}{An estimator generates probabilistic predictions for any given true values (blue line), based on data (not shown). The estimator has an inherent propensity for error (red curve). For any given true value the central value of a probabilistic prediction (black curve) may not match the true value, and if this is consistently over many predictions, the estimator may be regarded as biased. The estimator also generates a measure of uncertainty of the probabilistic prediction (black bars). Over many predictions the distribution of this uncertainty is a measure of estimator sharpness (yellow curve). The sharpness ideally matches the spread of the estimator's error function. If, as is shown here, the estimators sharpness is narrower than the error function spread then any single prediction is likely to be over-confident. This is the result of a mis-calibrated and over confident estimator. The ratio of the error function standard deviation and the median of the estimator spread distribution (sharpness) is the true value of the calibration}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods}{2}{section.2}\protected@file@percent }
\newlabel{eq:crps}{{{1}}{2}{Methods}{AMS.1}{}}
\citation{gneiting2007,david1948}
\citation{brockwell2007}
\citation{hamill2001,anderson1996}
\newlabel{eq:bias}{{{2}}{3}{Methods}{AMS.2}{}}
\newlabel{eq:ubias}{{{3}}{3}{Methods}{AMS.3}{}}
\newlabel{eq:sharp}{{{4}}{3}{Methods}{AMS.4}{}}
\newlabel{eq:cover}{{{5}}{3}{Methods}{AMS.5}{}}
\citation{panaretos2019}
\newlabel{eq:pit2}{{{6}}{4}{Methods}{AMS.6}{}}
\newlabel{eq:pit3}{{{7}}{4}{Methods}{AMS.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Validation}{4}{subsection.2.1}\protected@file@percent }
\newlabel{eq:valid}{{{8}}{4}{Validation}{AMS.8}{}}
\newlabel{eq:valid2}{{{9}}{5}{Validation}{AMS.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Results and Discussion}{5}{subsection.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Curated parameters for 9 simulated estimators with fixed sharpness and variable calibration and bias.}}{5}{table.1}\protected@file@percent }
\newlabel{tab:S2.1}{{1}{5}{Curated parameters for 9 simulated estimators with fixed sharpness and variable calibration and bias}{table.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces PIT histograms from 10000 simulated predictions from 9 simulated estimators with curated parameters, as given in Table\nobreakspace  {}\ref  {tab:S2.1}. Typical U and dome shaped patterns are seen as a result of mis-calibration, and skew as a result of estimator bias}}{6}{figure.2}\protected@file@percent }
\newlabel{fig:S2.2}{{2}{6}{PIT histograms from 10000 simulated predictions from 9 simulated estimators with curated parameters, as given in Table~\ref {tab:S2.1}. Typical U and dome shaped patterns are seen as a result of mis-calibration, and skew as a result of estimator bias}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Comparison of four estimator metrics for 1000 random simulated estimators, consisting of 10000 synthetic predications, as it relates to the theoretical calibration of the estimator on the X-axes. Panel A shows the mean CRPS, panel B the 50\% coverage probability. Panels C and D show adjusted and directed PIT-Wasserstein metrics, as described earlier in this paper, which are designed to give a measure of calibration that is independent of estimator bias and sharpness.}}{7}{figure.3}\protected@file@percent }
\newlabel{fig:S2.3}{{3}{7}{Comparison of four estimator metrics for 1000 random simulated estimators, consisting of 10000 synthetic predications, as it relates to the theoretical calibration of the estimator on the X-axes. Panel A shows the mean CRPS, panel B the 50\% coverage probability. Panels C and D show adjusted and directed PIT-Wasserstein metrics, as described earlier in this paper, which are designed to give a measure of calibration that is independent of estimator bias and sharpness}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A repeated comparison of traditional probabilistic estimator metrics, mean CRPS (Panel A) and 50\% coverage probability (Panel B) in the restricted scenario where simulated estimators are unbiased. In the absence of bias, the 50\% coverage probability is a good representation of calibration. The mean CRPS also is affected by sharpness so has unresolved interactions.}}{8}{figure.4}\protected@file@percent }
\newlabel{fig:S2.4}{{4}{8}{A repeated comparison of traditional probabilistic estimator metrics, mean CRPS (Panel A) and 50\% coverage probability (Panel B) in the restricted scenario where simulated estimators are unbiased. In the absence of bias, the 50\% coverage probability is a good representation of calibration. The mean CRPS also is affected by sharpness so has unresolved interactions}{figure.4}{}}
\bibdata{../main/refs}
\bibcite{anderson1996}{{1}{}{{}}{{}}}
\bibcite{bosse2024}{{2}{}{{}}{{}}}
\bibcite{bosse2023}{{3}{}{{}}{{}}}
\bibcite{brocker2008}{{4}{}{{}}{{}}}
\bibcite{gneiting2007}{{5}{}{{}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Conclusion}{9}{section.3}\protected@file@percent }
\bibcite{david1948}{{6}{}{{}}{{}}}
\bibcite{brockwell2007}{{7}{}{{}}{{}}}
\bibcite{hamill2001}{{8}{}{{}}{{}}}
\bibcite{panaretos2019}{{9}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\gdef \@abspage@last{10}
